--- Running 5-Fold Cross-Validation ---
Running Fold 1/5...
Fold 1: Training on 4010 samples.
      Epoch 50/1000, Total Loss: 92322.31
      Epoch 100/1000, Total Loss: 83538.61
      Epoch 150/1000, Total Loss: 79427.98
      Epoch 200/1000, Total Loss: 77246.45
      Epoch 250/1000, Total Loss: 75421.83
      Epoch 300/1000, Total Loss: 74241.96
      Epoch 350/1000, Total Loss: 73403.97
      Epoch 400/1000, Total Loss: 72625.69
      Epoch 450/1000, Total Loss: 72110.64
      Epoch 500/1000, Total Loss: 71725.52
      Epoch 550/1000, Total Loss: 71294.16
      Epoch 600/1000, Total Loss: 71039.80
      Epoch 650/1000, Total Loss: 70792.16
      Epoch 700/1000, Total Loss: 70484.88
      Epoch 750/1000, Total Loss: 70336.65
      Epoch 800/1000, Total Loss: 70201.36
      Epoch 850/1000, Total Loss: 69953.06
      Epoch 900/1000, Total Loss: 69897.97
      Epoch 950/1000, Total Loss: 69673.51
      Epoch 1000/1000, Total Loss: 69527.52
Latent mu mean: -0.00047026597894728184
Latent mu std: 0.2565418779850006
Running Fold 2/5...
Fold 2: Training on 4010 samples.
      Epoch 50/1000, Total Loss: 91031.41
      Epoch 100/1000, Total Loss: 82535.02
      Epoch 150/1000, Total Loss: 78372.01
      Epoch 200/1000, Total Loss: 75915.08
      Epoch 250/1000, Total Loss: 74395.61
      Epoch 300/1000, Total Loss: 73234.42
      Epoch 350/1000, Total Loss: 72351.04
      Epoch 400/1000, Total Loss: 71939.84
      Epoch 450/1000, Total Loss: 71295.11
      Epoch 500/1000, Total Loss: 70863.71
      Epoch 550/1000, Total Loss: 70703.27
      Epoch 600/1000, Total Loss: 70319.98
      Epoch 650/1000, Total Loss: 70176.15
      Epoch 700/1000, Total Loss: 69937.28
      Epoch 750/1000, Total Loss: 69691.89
      Epoch 800/1000, Total Loss: 69582.96
      Epoch 850/1000, Total Loss: 69537.01
      Epoch 900/1000, Total Loss: 69348.97
      Epoch 950/1000, Total Loss: 69232.92
      Epoch 1000/1000, Total Loss: 69010.45
Latent mu mean: 9.448336641071364e-05
Latent mu std: 0.26646688580513
Running Fold 3/5...
Fold 3: Training on 4010 samples.
      Epoch 50/1000, Total Loss: 92226.01
      Epoch 100/1000, Total Loss: 83361.77
      Epoch 150/1000, Total Loss: 79128.44
      Epoch 200/1000, Total Loss: 76813.59
      Epoch 250/1000, Total Loss: 75173.98
      Epoch 300/1000, Total Loss: 74021.55
      Epoch 350/1000, Total Loss: 73278.14
      Epoch 400/1000, Total Loss: 72423.34
      Epoch 450/1000, Total Loss: 71942.82
      Epoch 500/1000, Total Loss: 71580.08
      Epoch 550/1000, Total Loss: 71269.86
      Epoch 600/1000, Total Loss: 71020.69
      Epoch 650/1000, Total Loss: 70622.89
      Epoch 700/1000, Total Loss: 70504.56
      Epoch 750/1000, Total Loss: 70196.24
      Epoch 800/1000, Total Loss: 69915.73
      Epoch 850/1000, Total Loss: 70009.91
      Epoch 900/1000, Total Loss: 69728.63
      Epoch 950/1000, Total Loss: 69556.10
      Epoch 1000/1000, Total Loss: 69444.77
Latent mu mean: 0.0001491162256570533
Latent mu std: 0.25973159074783325
Running Fold 4/5...
Fold 4: Training on 4011 samples.
      Epoch 50/1000, Total Loss: 92338.62
      Epoch 100/1000, Total Loss: 83133.55
      Epoch 150/1000, Total Loss: 79156.17
      Epoch 200/1000, Total Loss: 76563.02
      Epoch 250/1000, Total Loss: 75025.74
      Epoch 300/1000, Total Loss: 73803.88
      Epoch 350/1000, Total Loss: 72785.85
      Epoch 400/1000, Total Loss: 72201.25
      Epoch 450/1000, Total Loss: 71698.06
      Epoch 500/1000, Total Loss: 71234.33
      Epoch 550/1000, Total Loss: 70932.88
      Epoch 600/1000, Total Loss: 70584.14
      Epoch 650/1000, Total Loss: 70395.96
      Epoch 700/1000, Total Loss: 70102.84
      Epoch 750/1000, Total Loss: 69963.28
      Epoch 800/1000, Total Loss: 69773.55
      Epoch 850/1000, Total Loss: 69559.94
      Epoch 900/1000, Total Loss: 69422.60
      Epoch 950/1000, Total Loss: 69276.15
      Epoch 1000/1000, Total Loss: 69093.30
Latent mu mean: -7.573462789878249e-05
Latent mu std: 0.2535989582538605
Running Fold 5/5...
Fold 5: Training on 4011 samples.
      Epoch 50/1000, Total Loss: 94264.83
      Epoch 100/1000, Total Loss: 85631.25
      Epoch 150/1000, Total Loss: 80744.73
      Epoch 200/1000, Total Loss: 78054.14
      Epoch 250/1000, Total Loss: 76285.32
      Epoch 300/1000, Total Loss: 74885.27
      Epoch 350/1000, Total Loss: 73920.49
      Epoch 400/1000, Total Loss: 73153.77
      Epoch 450/1000, Total Loss: 72679.95
      Epoch 500/1000, Total Loss: 72257.26
      Epoch 550/1000, Total Loss: 71753.59
      Epoch 600/1000, Total Loss: 71385.98
      Epoch 650/1000, Total Loss: 71156.44
      Epoch 700/1000, Total Loss: 70884.22
      Epoch 750/1000, Total Loss: 70634.87
      Epoch 800/1000, Total Loss: 70466.24
      Epoch 850/1000, Total Loss: 70311.20
      Epoch 900/1000, Total Loss: 70025.66
      Epoch 950/1000, Total Loss: 69970.05
      Epoch 1000/1000, Total Loss: 69819.62
Latent mu mean: -0.00037280184915289283
Latent mu std: 0.27247411012649536

Analysis finished in 116.45 seconds.

--- Results for Lag: 0 ms ---

1. VAE Shared Latent Space Encoding:
   Mean Correlation (r) = 0.2732 (+/- 0.0231)

2. VAE Denoising vs. Original Data Encoding:
   Subject 1:
     - Original Corr (r) : 0.3153
     - Denoised Corr (r) : 0.3302
     - Improvement       : 4.72%
   Subject 2:
     - Original Corr (r) : 0.2181
     - Denoised Corr (r) : 0.2322
     - Improvement       : 6.47%
   Subject 3:
     - Original Corr (r) : 0.2165
     - Denoised Corr (r) : 0.2213
     - Improvement       : 2.18%
   Subject 4:
     - Original Corr (r) : 0.1947
     - Denoised Corr (r) : 0.1935
     - Improvement       : -0.61%
   Subject 5:
     - Original Corr (r) : 0.2323
     - Denoised Corr (r) : 0.2603
     - Improvement       : 12.09%
   Subject 6:
     - Original Corr (r) : 0.3144
     - Denoised Corr (r) : 0.3279
     - Improvement       : 4.29%
   Subject 7:
     - Original Corr (r) : 0.2248
     - Denoised Corr (r) : 0.2046
     - Improvement       : -9.00%
   Subject 8:
     - Original Corr (r) : 0.2219
     - Denoised Corr (r) : 0.1949
     - Improvement       : -12.15%

--- Script Finished ---
