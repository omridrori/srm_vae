--- Running 5-Fold Cross-Validation ---
Running Fold 1/5...
Fold 1: Training on 4010 samples.
      Epoch 50/1000, Total Loss: 74380.26
      Epoch 100/1000, Total Loss: 71119.74
      Epoch 150/1000, Total Loss: 69798.98
      Epoch 200/1000, Total Loss: 69134.40
      Epoch 250/1000, Total Loss: 68650.71
      Epoch 300/1000, Total Loss: 68326.18
      Epoch 350/1000, Total Loss: 67977.84
      Epoch 400/1000, Total Loss: 67664.77
      Epoch 450/1000, Total Loss: 67497.43
      Epoch 500/1000, Total Loss: 67335.19
      Epoch 550/1000, Total Loss: 67251.83
      Epoch 600/1000, Total Loss: 66993.89
      Epoch 650/1000, Total Loss: 66903.84
      Epoch 700/1000, Total Loss: 66714.03
      Epoch 750/1000, Total Loss: 66724.81
      Epoch 800/1000, Total Loss: 66498.12
      Epoch 850/1000, Total Loss: 66481.27
      Epoch 900/1000, Total Loss: 66314.21
      Epoch 950/1000, Total Loss: 66215.80
      Epoch 1000/1000, Total Loss: 66134.44
Latent mu mean: -0.0008205727790482342
Latent mu std: 0.26590362191200256
Running Fold 2/5...
Fold 2: Training on 4010 samples.
      Epoch 50/1000, Total Loss: 72838.04
      Epoch 100/1000, Total Loss: 70090.34
      Epoch 150/1000, Total Loss: 69080.61
      Epoch 200/1000, Total Loss: 68583.48
      Epoch 250/1000, Total Loss: 68000.02
      Epoch 300/1000, Total Loss: 67683.39
      Epoch 350/1000, Total Loss: 67454.10
      Epoch 400/1000, Total Loss: 67240.02
      Epoch 450/1000, Total Loss: 67037.05
      Epoch 500/1000, Total Loss: 66839.80
      Epoch 550/1000, Total Loss: 66723.95
      Epoch 600/1000, Total Loss: 66659.08
      Epoch 650/1000, Total Loss: 66458.62
      Epoch 700/1000, Total Loss: 66347.09
      Epoch 750/1000, Total Loss: 66262.75
      Epoch 800/1000, Total Loss: 66180.48
      Epoch 850/1000, Total Loss: 66042.18
      Epoch 900/1000, Total Loss: 65970.35
      Epoch 950/1000, Total Loss: 65824.53
      Epoch 1000/1000, Total Loss: 65711.50
Latent mu mean: -0.004130570683628321
Latent mu std: 0.2625705599784851
Running Fold 3/5...
Fold 3: Training on 4010 samples.
      Epoch 50/1000, Total Loss: 73696.51
      Epoch 100/1000, Total Loss: 70583.88
      Epoch 150/1000, Total Loss: 69436.38
      Epoch 200/1000, Total Loss: 68752.72
      Epoch 250/1000, Total Loss: 68407.64
      Epoch 300/1000, Total Loss: 67975.31
      Epoch 350/1000, Total Loss: 67673.45
      Epoch 400/1000, Total Loss: 67546.86
      Epoch 450/1000, Total Loss: 67210.90
      Epoch 500/1000, Total Loss: 67115.12
      Epoch 550/1000, Total Loss: 66978.18
      Epoch 600/1000, Total Loss: 66790.00
      Epoch 650/1000, Total Loss: 66699.62
      Epoch 700/1000, Total Loss: 66518.77
      Epoch 750/1000, Total Loss: 66389.49
      Epoch 800/1000, Total Loss: 66349.61
      Epoch 850/1000, Total Loss: 66145.32
      Epoch 900/1000, Total Loss: 66136.00
      Epoch 950/1000, Total Loss: 66027.84
      Epoch 1000/1000, Total Loss: 65936.82
Latent mu mean: 0.0006387335597537458
Latent mu std: 0.2536433935165405
Running Fold 4/5...
Fold 4: Training on 4011 samples.
      Epoch 50/1000, Total Loss: 72788.95
      Epoch 100/1000, Total Loss: 70226.16
      Epoch 150/1000, Total Loss: 69187.87
      Epoch 200/1000, Total Loss: 68574.06
      Epoch 250/1000, Total Loss: 68070.27
      Epoch 300/1000, Total Loss: 67897.75
      Epoch 350/1000, Total Loss: 67513.77
      Epoch 400/1000, Total Loss: 67323.28
      Epoch 450/1000, Total Loss: 67209.14
      Epoch 500/1000, Total Loss: 66970.04
      Epoch 550/1000, Total Loss: 66834.77
      Epoch 600/1000, Total Loss: 66711.31
      Epoch 650/1000, Total Loss: 66550.52
      Epoch 700/1000, Total Loss: 66294.26
      Epoch 750/1000, Total Loss: 66318.57
      Epoch 800/1000, Total Loss: 66168.45
      Epoch 850/1000, Total Loss: 66051.83
      Epoch 900/1000, Total Loss: 66033.98
      Epoch 950/1000, Total Loss: 65879.03
      Epoch 1000/1000, Total Loss: 65805.61
Latent mu mean: 0.0014660960296168923
Latent mu std: 0.26603344082832336
Running Fold 5/5...
Fold 5: Training on 4011 samples.
      Epoch 50/1000, Total Loss: 73582.94
      Epoch 100/1000, Total Loss: 70600.89
      Epoch 150/1000, Total Loss: 69442.78
      Epoch 200/1000, Total Loss: 68916.15
      Epoch 250/1000, Total Loss: 68411.54
      Epoch 300/1000, Total Loss: 67970.62
      Epoch 350/1000, Total Loss: 67673.37
      Epoch 400/1000, Total Loss: 67478.45
      Epoch 450/1000, Total Loss: 67265.30
      Epoch 500/1000, Total Loss: 67080.97
      Epoch 550/1000, Total Loss: 66979.71
      Epoch 600/1000, Total Loss: 66890.98
      Epoch 650/1000, Total Loss: 66723.16
      Epoch 700/1000, Total Loss: 66616.18
      Epoch 750/1000, Total Loss: 66523.56
      Epoch 800/1000, Total Loss: 66469.36
      Epoch 850/1000, Total Loss: 66293.37
      Epoch 900/1000, Total Loss: 66167.97
      Epoch 950/1000, Total Loss: 66184.84
      Epoch 1000/1000, Total Loss: 66057.68
Latent mu mean: -0.0013990683946758509
Latent mu std: 0.259162962436676

Analysis finished in 104.07 seconds.

--- Results for Lag: 0 ms ---

1. VAE Shared Latent Space Encoding:
   Mean Correlation (r) = 0.2772 (+/- 0.0261)

2. VAE Denoising vs. Original Data Encoding:
   Subject 1:
     - Original Corr (r) : 0.3165
     - Denoised Corr (r) : 0.3380
     - Improvement       : 6.79%
   Subject 2:
     - Original Corr (r) : 0.2181
     - Denoised Corr (r) : 0.2329
     - Improvement       : 6.76%
   Subject 3:
     - Original Corr (r) : 0.2161
     - Denoised Corr (r) : 0.2216
     - Improvement       : 2.56%
   Subject 4:
     - Original Corr (r) : 0.1948
     - Denoised Corr (r) : 0.1906
     - Improvement       : -2.15%
   Subject 5:
     - Original Corr (r) : 0.2334
     - Denoised Corr (r) : 0.2519
     - Improvement       : 7.94%
   Subject 6:
     - Original Corr (r) : 0.3161
     - Denoised Corr (r) : 0.3270
     - Improvement       : 3.42%
   Subject 7:
     - Original Corr (r) : 0.2257
     - Denoised Corr (r) : 0.2027
     - Improvement       : -10.17%
   Subject 8:
     - Original Corr (r) : 0.2220
     - Denoised Corr (r) : 0.1837
     - Improvement       : -17.23%

--- Script Finished ---
