--- Running 5-Fold Cross-Validation ---
Running Fold 1/5...
Fold 1: Training on 4010 samples.
      Epoch 50/1000, Total Loss: 74097.37
      Epoch 100/1000, Total Loss: 71113.44
      Epoch 150/1000, Total Loss: 69726.41
      Epoch 200/1000, Total Loss: 69107.77
      Epoch 250/1000, Total Loss: 68676.17
      Epoch 300/1000, Total Loss: 68236.13
      Epoch 350/1000, Total Loss: 67911.47
      Epoch 400/1000, Total Loss: 67629.84
      Epoch 450/1000, Total Loss: 67473.70
      Epoch 500/1000, Total Loss: 67273.39
      Epoch 550/1000, Total Loss: 67109.44
      Epoch 600/1000, Total Loss: 66947.36
      Epoch 650/1000, Total Loss: 66904.87
      Epoch 700/1000, Total Loss: 66583.77
      Epoch 750/1000, Total Loss: 66614.91
      Epoch 800/1000, Total Loss: 66385.45
      Epoch 850/1000, Total Loss: 66314.19
      Epoch 900/1000, Total Loss: 66271.70
      Epoch 950/1000, Total Loss: 66197.74
      Epoch 1000/1000, Total Loss: 66114.60
Latent mu mean: 0.0005889747990295291
Latent mu std: 0.2702718675136566
Running Fold 2/5...
Fold 2: Training on 4010 samples.
      Epoch 50/1000, Total Loss: 73012.77
      Epoch 100/1000, Total Loss: 70307.84
      Epoch 150/1000, Total Loss: 69206.41
      Epoch 200/1000, Total Loss: 68598.38
      Epoch 250/1000, Total Loss: 68131.88
      Epoch 300/1000, Total Loss: 67761.15
      Epoch 350/1000, Total Loss: 67498.16
      Epoch 400/1000, Total Loss: 67270.26
      Epoch 450/1000, Total Loss: 67167.18
      Epoch 500/1000, Total Loss: 66958.09
      Epoch 550/1000, Total Loss: 66862.12
      Epoch 600/1000, Total Loss: 66641.80
      Epoch 650/1000, Total Loss: 66536.80
      Epoch 700/1000, Total Loss: 66354.20
      Epoch 750/1000, Total Loss: 66221.27
      Epoch 800/1000, Total Loss: 66291.84
      Epoch 850/1000, Total Loss: 66129.91
      Epoch 900/1000, Total Loss: 66106.70
      Epoch 950/1000, Total Loss: 65793.10
      Epoch 1000/1000, Total Loss: 65919.27
Latent mu mean: -0.0014913312625139952
Latent mu std: 0.2679315507411957
Running Fold 3/5...
Fold 3: Training on 4010 samples.
      Epoch 50/1000, Total Loss: 72881.07
      Epoch 100/1000, Total Loss: 70289.55
      Epoch 150/1000, Total Loss: 69338.98
      Epoch 200/1000, Total Loss: 68822.50
      Epoch 250/1000, Total Loss: 68324.50
      Epoch 300/1000, Total Loss: 67843.31
      Epoch 350/1000, Total Loss: 67628.84
      Epoch 400/1000, Total Loss: 67358.15
      Epoch 450/1000, Total Loss: 67119.46
      Epoch 500/1000, Total Loss: 67000.62
      Epoch 550/1000, Total Loss: 66863.87
      Epoch 600/1000, Total Loss: 66655.84
      Epoch 650/1000, Total Loss: 66554.25
      Epoch 700/1000, Total Loss: 66461.52
      Epoch 750/1000, Total Loss: 66219.25
      Epoch 800/1000, Total Loss: 66246.94
      Epoch 850/1000, Total Loss: 66053.05
      Epoch 900/1000, Total Loss: 66161.06
      Epoch 950/1000, Total Loss: 65906.03
      Epoch 1000/1000, Total Loss: 65927.73
Latent mu mean: 0.0003892685053870082
Latent mu std: 0.27248862385749817
Running Fold 4/5...
Fold 4: Training on 4011 samples.
      Epoch 50/1000, Total Loss: 73118.95
      Epoch 100/1000, Total Loss: 70339.71
      Epoch 150/1000, Total Loss: 69322.30
      Epoch 200/1000, Total Loss: 68777.36
      Epoch 250/1000, Total Loss: 68357.73
      Epoch 300/1000, Total Loss: 67911.66
      Epoch 350/1000, Total Loss: 67597.48
      Epoch 400/1000, Total Loss: 67346.18
      Epoch 450/1000, Total Loss: 67249.14
      Epoch 500/1000, Total Loss: 66906.43
      Epoch 550/1000, Total Loss: 66915.75
      Epoch 600/1000, Total Loss: 66735.39
      Epoch 650/1000, Total Loss: 66515.24
      Epoch 700/1000, Total Loss: 66457.55
      Epoch 750/1000, Total Loss: 66371.77
      Epoch 800/1000, Total Loss: 66251.04
      Epoch 850/1000, Total Loss: 66175.09
      Epoch 900/1000, Total Loss: 66144.47
      Epoch 950/1000, Total Loss: 65998.23
      Epoch 1000/1000, Total Loss: 65926.64
Latent mu mean: 0.0007382850162684917
Latent mu std: 0.25944703817367554
Running Fold 5/5...
Fold 5: Training on 4011 samples.
      Epoch 50/1000, Total Loss: 73578.00
      Epoch 100/1000, Total Loss: 70567.30
      Epoch 150/1000, Total Loss: 69399.19
      Epoch 200/1000, Total Loss: 68704.19
      Epoch 250/1000, Total Loss: 68314.42
      Epoch 300/1000, Total Loss: 67958.67
      Epoch 350/1000, Total Loss: 67660.70
      Epoch 400/1000, Total Loss: 67485.60
      Epoch 450/1000, Total Loss: 67187.12
      Epoch 500/1000, Total Loss: 67116.02
      Epoch 550/1000, Total Loss: 66926.17
      Epoch 600/1000, Total Loss: 66746.45
      Epoch 650/1000, Total Loss: 66702.71
      Epoch 700/1000, Total Loss: 66565.14
      Epoch 750/1000, Total Loss: 66494.12
      Epoch 800/1000, Total Loss: 66235.80
      Epoch 850/1000, Total Loss: 66254.34
      Epoch 900/1000, Total Loss: 66245.20
      Epoch 950/1000, Total Loss: 66103.42
      Epoch 1000/1000, Total Loss: 66016.14
Latent mu mean: -0.0029507491271942854
Latent mu std: 0.277511328458786

Analysis finished in 100.13 seconds.

--- Results for Lag: 0 ms ---

1. VAE Shared Latent Space Encoding:
   Mean Correlation (r) = 0.2878 (+/- 0.0150)

2. VAE Denoising vs. Original Data Encoding:
   Subject 1:
     - Original Corr (r) : 0.3169
     - Denoised Corr (r) : 0.3383
     - Improvement       : 6.77%
   Subject 2:
     - Original Corr (r) : 0.2197
     - Denoised Corr (r) : 0.2353
     - Improvement       : 7.11%
   Subject 3:
     - Original Corr (r) : 0.2164
     - Denoised Corr (r) : 0.2230
     - Improvement       : 3.07%
   Subject 4:
     - Original Corr (r) : 0.1962
     - Denoised Corr (r) : 0.1894
     - Improvement       : -3.44%
   Subject 5:
     - Original Corr (r) : 0.2339
     - Denoised Corr (r) : 0.2538
     - Improvement       : 8.50%
   Subject 6:
     - Original Corr (r) : 0.3178
     - Denoised Corr (r) : 0.3283
     - Improvement       : 3.32%
   Subject 7:
     - Original Corr (r) : 0.2262
     - Denoised Corr (r) : 0.2043
     - Improvement       : -9.68%
   Subject 8:
     - Original Corr (r) : 0.2225
     - Denoised Corr (r) : 0.1854
     - Improvement       : -16.71%

--- Script Finished ---
