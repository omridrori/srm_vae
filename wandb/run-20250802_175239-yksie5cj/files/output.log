--- Running 5-Fold Cross-Validation ---
Running Fold 1/5...
Fold 1: Training on 4010 samples.
      Epoch 50/1000, Total Loss: 82657.16
      Epoch 100/1000, Total Loss: 80493.12
      Epoch 150/1000, Total Loss: 79599.09
      Epoch 200/1000, Total Loss: 79087.44
      Epoch 250/1000, Total Loss: 78955.09
      Epoch 300/1000, Total Loss: 78604.48
      Epoch 350/1000, Total Loss: 78419.36
      Epoch 400/1000, Total Loss: 78133.31
      Epoch 450/1000, Total Loss: 78167.02
      Epoch 500/1000, Total Loss: 77909.73
      Epoch 550/1000, Total Loss: 77766.48
      Epoch 600/1000, Total Loss: 77641.32
      Epoch 650/1000, Total Loss: 77561.23
      Epoch 700/1000, Total Loss: 77457.54
      Epoch 750/1000, Total Loss: 77506.02
      Epoch 800/1000, Total Loss: 77402.46
      Epoch 850/1000, Total Loss: 77385.40
      Epoch 900/1000, Total Loss: 77336.52
      Epoch 950/1000, Total Loss: 77209.99
      Epoch 1000/1000, Total Loss: 77155.85
Latent mu mean: -0.00012042406888213009
Latent mu std: 0.16622324287891388
Running Fold 2/5...
Fold 2: Training on 4010 samples.
      Epoch 50/1000, Total Loss: 81566.88
      Epoch 100/1000, Total Loss: 79782.07
      Epoch 150/1000, Total Loss: 79036.30
      Epoch 200/1000, Total Loss: 78639.66
      Epoch 250/1000, Total Loss: 78333.05
      Epoch 300/1000, Total Loss: 78173.59
      Epoch 350/1000, Total Loss: 78107.80
      Epoch 400/1000, Total Loss: 77851.34
      Epoch 450/1000, Total Loss: 77652.57
      Epoch 500/1000, Total Loss: 77560.13
      Epoch 550/1000, Total Loss: 77434.58
      Epoch 600/1000, Total Loss: 77359.27
      Epoch 650/1000, Total Loss: 77266.03
      Epoch 700/1000, Total Loss: 77178.78
      Epoch 750/1000, Total Loss: 77089.29
      Epoch 800/1000, Total Loss: 77028.71
      Epoch 850/1000, Total Loss: 77031.92
      Epoch 900/1000, Total Loss: 76867.23
      Epoch 950/1000, Total Loss: 76903.37
      Epoch 1000/1000, Total Loss: 76942.42
Latent mu mean: -0.0007741769077256322
Latent mu std: 0.17862153053283691
Running Fold 3/5...
Fold 3: Training on 4010 samples.
      Epoch 50/1000, Total Loss: 81893.29
      Epoch 100/1000, Total Loss: 80052.00
      Epoch 150/1000, Total Loss: 79320.78
      Epoch 200/1000, Total Loss: 78875.69
      Epoch 250/1000, Total Loss: 78686.90
      Epoch 300/1000, Total Loss: 78308.92
      Epoch 350/1000, Total Loss: 78094.64
      Epoch 400/1000, Total Loss: 77853.88
      Epoch 450/1000, Total Loss: 77811.95
      Epoch 500/1000, Total Loss: 77633.77
      Epoch 550/1000, Total Loss: 77652.19
      Epoch 600/1000, Total Loss: 77437.91
      Epoch 650/1000, Total Loss: 77362.09
      Epoch 700/1000, Total Loss: 77169.52
      Epoch 750/1000, Total Loss: 77151.27
      Epoch 800/1000, Total Loss: 77096.70
      Epoch 850/1000, Total Loss: 77023.69
      Epoch 900/1000, Total Loss: 76992.46
      Epoch 950/1000, Total Loss: 76900.34
      Epoch 1000/1000, Total Loss: 76875.32
Latent mu mean: 5.193077595322393e-05
Latent mu std: 0.19031666219234467
Running Fold 4/5...
Fold 4: Training on 4011 samples.
      Epoch 50/1000, Total Loss: 82251.52
      Epoch 100/1000, Total Loss: 80099.22
      Epoch 150/1000, Total Loss: 79276.75
      Epoch 200/1000, Total Loss: 78814.61
      Epoch 250/1000, Total Loss: 78527.74
      Epoch 300/1000, Total Loss: 78241.23
      Epoch 350/1000, Total Loss: 78034.04
      Epoch 400/1000, Total Loss: 77822.51
      Epoch 450/1000, Total Loss: 77761.73
      Epoch 500/1000, Total Loss: 77647.16
      Epoch 550/1000, Total Loss: 77558.24
      Epoch 600/1000, Total Loss: 77484.29
      Epoch 650/1000, Total Loss: 77455.66
      Epoch 700/1000, Total Loss: 77256.43
      Epoch 750/1000, Total Loss: 77261.96
      Epoch 800/1000, Total Loss: 77093.59
      Epoch 850/1000, Total Loss: 77168.43
      Epoch 900/1000, Total Loss: 77053.01
      Epoch 950/1000, Total Loss: 76945.21
      Epoch 1000/1000, Total Loss: 76977.87
Latent mu mean: 0.0009664864046499133
Latent mu std: 0.15873603522777557
Running Fold 5/5...
Fold 5: Training on 4011 samples.
      Epoch 50/1000, Total Loss: 82330.98
      Epoch 100/1000, Total Loss: 80414.42
      Epoch 150/1000, Total Loss: 79634.25
      Epoch 200/1000, Total Loss: 79028.93
      Epoch 250/1000, Total Loss: 78637.91
      Epoch 300/1000, Total Loss: 78395.52
      Epoch 350/1000, Total Loss: 78198.49
      Epoch 400/1000, Total Loss: 77835.05
      Epoch 450/1000, Total Loss: 77929.60
      Epoch 500/1000, Total Loss: 77789.79
      Epoch 550/1000, Total Loss: 77623.25
      Epoch 600/1000, Total Loss: 77544.94
      Epoch 650/1000, Total Loss: 77403.30
      Epoch 700/1000, Total Loss: 77389.08
      Epoch 750/1000, Total Loss: 77334.39
      Epoch 800/1000, Total Loss: 77212.42
      Epoch 850/1000, Total Loss: 77073.30
      Epoch 900/1000, Total Loss: 77245.28
      Epoch 950/1000, Total Loss: 77089.30
      Epoch 1000/1000, Total Loss: 77083.41
Latent mu mean: 0.0002091656206175685
Latent mu std: 0.21597102284431458

Analysis finished in 104.67 seconds.

--- Results for Lag: 0 ms ---

1. VAE Shared Latent Space Encoding:
   Mean Correlation (r) = 0.3082 (+/- 0.0480)

2. VAE Denoising vs. Original Data Encoding:
   Subject 1:
     - Original Corr (r) : 0.3158
     - Denoised Corr (r) : 0.2897
     - Improvement       : -8.26%
   Subject 2:
     - Original Corr (r) : 0.2183
     - Denoised Corr (r) : 0.2270
     - Improvement       : 3.97%
   Subject 3:
     - Original Corr (r) : 0.2170
     - Denoised Corr (r) : 0.1712
     - Improvement       : -21.10%
   Subject 4:
     - Original Corr (r) : 0.1947
     - Denoised Corr (r) : 0.1420
     - Improvement       : -27.08%
   Subject 5:
     - Original Corr (r) : 0.2333
     - Denoised Corr (r) : 0.2537
     - Improvement       : 8.76%
   Subject 6:
     - Original Corr (r) : 0.3162
     - Denoised Corr (r) : 0.3187
     - Improvement       : 0.80%
   Subject 7:
     - Original Corr (r) : 0.2270
     - Denoised Corr (r) : 0.1689
     - Improvement       : -25.62%
   Subject 8:
     - Original Corr (r) : 0.2219
     - Denoised Corr (r) : 0.1646
     - Improvement       : -25.83%

--- Script Finished ---
