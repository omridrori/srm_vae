--- Running 5-Fold Cross-Validation ---
Running Fold 1/5...
Fold 1: Training on 4010 samples.
      Epoch 50/1000, Total Loss: 82404.00
      Epoch 100/1000, Total Loss: 80465.41
      Epoch 150/1000, Total Loss: 79707.22
      Epoch 200/1000, Total Loss: 79413.20
      Epoch 250/1000, Total Loss: 79168.96
      Epoch 300/1000, Total Loss: 78799.05
      Epoch 350/1000, Total Loss: 78661.76
      Epoch 400/1000, Total Loss: 78405.83
      Epoch 450/1000, Total Loss: 78232.55
      Epoch 500/1000, Total Loss: 78138.76
      Epoch 550/1000, Total Loss: 78058.36
      Epoch 600/1000, Total Loss: 77835.05
      Epoch 650/1000, Total Loss: 77757.88
      Epoch 700/1000, Total Loss: 77791.67
      Epoch 750/1000, Total Loss: 77666.15
      Epoch 800/1000, Total Loss: 77537.48
      Epoch 850/1000, Total Loss: 77577.37
      Epoch 900/1000, Total Loss: 77528.12
      Epoch 950/1000, Total Loss: 77440.60
      Epoch 1000/1000, Total Loss: 77381.52
Latent mu mean: 1.789451198419556e-05
Latent mu std: 0.19617193937301636
Running Fold 2/5...
Fold 2: Training on 4010 samples.
      Epoch 50/1000, Total Loss: 81111.73
      Epoch 100/1000, Total Loss: 79745.83
      Epoch 150/1000, Total Loss: 79391.09
      Epoch 200/1000, Total Loss: 79024.05
      Epoch 250/1000, Total Loss: 78749.21
      Epoch 300/1000, Total Loss: 78527.32
      Epoch 350/1000, Total Loss: 78327.15
      Epoch 400/1000, Total Loss: 78138.80
      Epoch 450/1000, Total Loss: 78055.72
      Epoch 500/1000, Total Loss: 78029.76
      Epoch 550/1000, Total Loss: 77797.20
      Epoch 600/1000, Total Loss: 77751.54
      Epoch 650/1000, Total Loss: 77686.62
      Epoch 700/1000, Total Loss: 77648.42
      Epoch 750/1000, Total Loss: 77606.92
      Epoch 800/1000, Total Loss: 77530.98
      Epoch 850/1000, Total Loss: 77463.80
      Epoch 900/1000, Total Loss: 77472.56
      Epoch 950/1000, Total Loss: 77338.77
      Epoch 1000/1000, Total Loss: 77216.99
Latent mu mean: 0.0018275154288858175
Latent mu std: 0.2178523987531662
Running Fold 3/5...
Fold 3: Training on 4010 samples.
      Epoch 50/1000, Total Loss: 81224.36
      Epoch 100/1000, Total Loss: 79889.10
      Epoch 150/1000, Total Loss: 79425.30
      Epoch 200/1000, Total Loss: 78967.62
      Epoch 250/1000, Total Loss: 78688.02
      Epoch 300/1000, Total Loss: 78438.02
      Epoch 350/1000, Total Loss: 78288.77
      Epoch 400/1000, Total Loss: 78227.40
      Epoch 450/1000, Total Loss: 77948.47
      Epoch 500/1000, Total Loss: 77861.49
      Epoch 550/1000, Total Loss: 77784.82
      Epoch 600/1000, Total Loss: 77707.73
      Epoch 650/1000, Total Loss: 77667.98
      Epoch 700/1000, Total Loss: 77604.04
      Epoch 750/1000, Total Loss: 77432.17
      Epoch 800/1000, Total Loss: 77491.52
      Epoch 850/1000, Total Loss: 77381.80
      Epoch 900/1000, Total Loss: 77343.38
      Epoch 950/1000, Total Loss: 77294.41
      Epoch 1000/1000, Total Loss: 77301.73
Latent mu mean: -0.0014759856276214123
Latent mu std: 0.2227030247449875
Running Fold 4/5...
Fold 4: Training on 4011 samples.
      Epoch 50/1000, Total Loss: 81247.30
      Epoch 100/1000, Total Loss: 79853.62
      Epoch 150/1000, Total Loss: 79248.30
      Epoch 200/1000, Total Loss: 78829.94
      Epoch 250/1000, Total Loss: 78476.16
      Epoch 300/1000, Total Loss: 78169.65
      Epoch 350/1000, Total Loss: 78022.43
      Epoch 400/1000, Total Loss: 78001.23
      Epoch 450/1000, Total Loss: 77942.75
      Epoch 500/1000, Total Loss: 77753.77
      Epoch 550/1000, Total Loss: 77700.15
      Epoch 600/1000, Total Loss: 77522.51
      Epoch 650/1000, Total Loss: 77497.67
      Epoch 700/1000, Total Loss: 77436.88
      Epoch 750/1000, Total Loss: 77352.80
      Epoch 800/1000, Total Loss: 77345.55
      Epoch 850/1000, Total Loss: 77239.60
      Epoch 900/1000, Total Loss: 77138.31
      Epoch 950/1000, Total Loss: 77226.44
      Epoch 1000/1000, Total Loss: 77054.95
Latent mu mean: -0.00010546054545557126
Latent mu std: 0.2172853797674179
Running Fold 5/5...
Fold 5: Training on 4011 samples.
      Epoch 50/1000, Total Loss: 81733.80
      Epoch 100/1000, Total Loss: 80195.55
      Epoch 150/1000, Total Loss: 79594.59
      Epoch 200/1000, Total Loss: 79140.45
      Epoch 250/1000, Total Loss: 78978.96
      Epoch 300/1000, Total Loss: 78484.63
      Epoch 350/1000, Total Loss: 78534.61
      Epoch 400/1000, Total Loss: 78231.23
      Epoch 450/1000, Total Loss: 78104.02
      Epoch 500/1000, Total Loss: 78111.41
      Epoch 550/1000, Total Loss: 77971.22
      Epoch 600/1000, Total Loss: 77899.93
      Epoch 650/1000, Total Loss: 77723.73
      Epoch 700/1000, Total Loss: 77708.92
      Epoch 750/1000, Total Loss: 77579.81
      Epoch 800/1000, Total Loss: 77615.95
      Epoch 850/1000, Total Loss: 77450.05
      Epoch 900/1000, Total Loss: 77482.51
      Epoch 950/1000, Total Loss: 77507.81
      Epoch 1000/1000, Total Loss: 77361.48
Latent mu mean: -0.0007488542469218373
Latent mu std: 0.23987887799739838

Analysis finished in 106.59 seconds.

--- Results for Lag: 0 ms ---

1. VAE Shared Latent Space Encoding:
   Mean Correlation (r) = 0.3031 (+/- 0.0309)

2. VAE Denoising vs. Original Data Encoding:
   Subject 1:
     - Original Corr (r) : 0.3174
     - Denoised Corr (r) : 0.2955
     - Improvement       : -6.90%
   Subject 2:
     - Original Corr (r) : 0.2183
     - Denoised Corr (r) : 0.2226
     - Improvement       : 1.98%
   Subject 3:
     - Original Corr (r) : 0.2165
     - Denoised Corr (r) : 0.1729
     - Improvement       : -20.13%
   Subject 4:
     - Original Corr (r) : 0.1957
     - Denoised Corr (r) : 0.1390
     - Improvement       : -28.98%
   Subject 5:
     - Original Corr (r) : 0.2333
     - Denoised Corr (r) : 0.2499
     - Improvement       : 7.10%
   Subject 6:
     - Original Corr (r) : 0.3166
     - Denoised Corr (r) : 0.3188
     - Improvement       : 0.69%
   Subject 7:
     - Original Corr (r) : 0.2262
     - Denoised Corr (r) : 0.1674
     - Improvement       : -26.00%
   Subject 8:
     - Original Corr (r) : 0.2222
     - Denoised Corr (r) : 0.1646
     - Improvement       : -25.93%

--- Script Finished ---
