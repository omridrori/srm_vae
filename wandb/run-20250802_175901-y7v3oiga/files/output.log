--- Running 5-Fold Cross-Validation ---
Running Fold 1/5...
Fold 1: Training on 4010 samples.
      Epoch 50/1000, Total Loss: 63736.03
      Epoch 100/1000, Total Loss: 60641.43
      Epoch 150/1000, Total Loss: 59168.44
      Epoch 200/1000, Total Loss: 58191.32
      Epoch 250/1000, Total Loss: 57394.74
      Epoch 300/1000, Total Loss: 56780.97
      Epoch 350/1000, Total Loss: 56185.43
      Epoch 400/1000, Total Loss: 55652.93
      Epoch 450/1000, Total Loss: 55162.90
      Epoch 500/1000, Total Loss: 54798.06
      Epoch 550/1000, Total Loss: 54384.73
      Epoch 600/1000, Total Loss: 54019.75
      Epoch 650/1000, Total Loss: 53694.41
      Epoch 700/1000, Total Loss: 53362.22
      Epoch 750/1000, Total Loss: 53061.00
      Epoch 800/1000, Total Loss: 52805.61
      Epoch 850/1000, Total Loss: 52549.85
      Epoch 900/1000, Total Loss: 52304.82
      Epoch 950/1000, Total Loss: 52081.00
      Epoch 1000/1000, Total Loss: 51928.93
Latent mu mean: -0.0019141756929457188
Latent mu std: 0.310867041349411
Running Fold 2/5...
Fold 2: Training on 4010 samples.
      Epoch 50/1000, Total Loss: 63506.35
      Epoch 100/1000, Total Loss: 60348.05
      Epoch 150/1000, Total Loss: 59028.40
      Epoch 200/1000, Total Loss: 58172.38
      Epoch 250/1000, Total Loss: 57440.99
      Epoch 300/1000, Total Loss: 56824.16
      Epoch 350/1000, Total Loss: 56270.11
      Epoch 400/1000, Total Loss: 55732.44
      Epoch 450/1000, Total Loss: 55323.88
      Epoch 500/1000, Total Loss: 54911.11
      Epoch 550/1000, Total Loss: 54520.27
      Epoch 600/1000, Total Loss: 54128.67
      Epoch 650/1000, Total Loss: 53739.53
      Epoch 700/1000, Total Loss: 53427.48
      Epoch 750/1000, Total Loss: 53139.10
      Epoch 800/1000, Total Loss: 52818.36
      Epoch 850/1000, Total Loss: 52583.21
      Epoch 900/1000, Total Loss: 52321.85
      Epoch 950/1000, Total Loss: 52042.24
      Epoch 1000/1000, Total Loss: 51831.46
Latent mu mean: -0.0019985605031251907
Latent mu std: 0.3588641285896301
Running Fold 3/5...
Fold 3: Training on 4010 samples.
      Epoch 50/1000, Total Loss: 63912.40
      Epoch 100/1000, Total Loss: 60594.95
      Epoch 150/1000, Total Loss: 59034.77
      Epoch 200/1000, Total Loss: 57991.50
      Epoch 250/1000, Total Loss: 57278.11
      Epoch 300/1000, Total Loss: 56566.56
      Epoch 350/1000, Total Loss: 56003.14
      Epoch 400/1000, Total Loss: 55517.24
      Epoch 450/1000, Total Loss: 55085.15
      Epoch 500/1000, Total Loss: 54708.92
      Epoch 550/1000, Total Loss: 54315.77
      Epoch 600/1000, Total Loss: 54039.41
      Epoch 650/1000, Total Loss: 53680.84
      Epoch 700/1000, Total Loss: 53368.89
      Epoch 750/1000, Total Loss: 53053.59
      Epoch 800/1000, Total Loss: 52833.32
      Epoch 850/1000, Total Loss: 52596.81
      Epoch 900/1000, Total Loss: 52257.02
      Epoch 950/1000, Total Loss: 52115.89
      Epoch 1000/1000, Total Loss: 51863.02
Latent mu mean: 0.0003069197991862893
Latent mu std: 0.3061952292919159
Running Fold 4/5...
Fold 4: Training on 4011 samples.
      Epoch 50/1000, Total Loss: 63582.24
      Epoch 100/1000, Total Loss: 60167.21
      Epoch 150/1000, Total Loss: 58983.25
      Epoch 200/1000, Total Loss: 58084.58
      Epoch 250/1000, Total Loss: 57296.70
      Epoch 300/1000, Total Loss: 56608.12
      Epoch 350/1000, Total Loss: 56054.25
      Epoch 400/1000, Total Loss: 55535.18
      Epoch 450/1000, Total Loss: 55086.88
      Epoch 500/1000, Total Loss: 54669.48
      Epoch 550/1000, Total Loss: 54239.46
      Epoch 600/1000, Total Loss: 53918.06
      Epoch 650/1000, Total Loss: 53571.02
      Epoch 700/1000, Total Loss: 53284.74
      Epoch 750/1000, Total Loss: 52913.64
      Epoch 800/1000, Total Loss: 52684.13
      Epoch 850/1000, Total Loss: 52431.76
      Epoch 900/1000, Total Loss: 52183.93
      Epoch 950/1000, Total Loss: 51944.79
      Epoch 1000/1000, Total Loss: 51804.26
Latent mu mean: -0.0018945540068671107
Latent mu std: 0.35871371626853943
Running Fold 5/5...
Fold 5: Training on 4011 samples.
      Epoch 50/1000, Total Loss: 63203.87
      Epoch 100/1000, Total Loss: 60104.59
      Epoch 150/1000, Total Loss: 58746.30
      Epoch 200/1000, Total Loss: 57832.21
      Epoch 250/1000, Total Loss: 57127.79
      Epoch 300/1000, Total Loss: 56483.39
      Epoch 350/1000, Total Loss: 55888.07
      Epoch 400/1000, Total Loss: 55449.28
      Epoch 450/1000, Total Loss: 54937.51
      Epoch 500/1000, Total Loss: 54545.30
      Epoch 550/1000, Total Loss: 54123.83
      Epoch 600/1000, Total Loss: 53816.08
      Epoch 650/1000, Total Loss: 53480.69
      Epoch 700/1000, Total Loss: 53230.84
      Epoch 750/1000, Total Loss: 52847.63
      Epoch 800/1000, Total Loss: 52564.22
      Epoch 850/1000, Total Loss: 52377.55
      Epoch 900/1000, Total Loss: 52053.97
      Epoch 950/1000, Total Loss: 51884.46
      Epoch 1000/1000, Total Loss: 51700.27
Latent mu mean: -0.00047197184176184237
Latent mu std: 0.3268525302410126

Analysis finished in 102.60 seconds.

--- Results for Lag: 0 ms ---

1. VAE Shared Latent Space Encoding:
   Mean Correlation (r) = 0.2754 (+/- 0.0453)

2. VAE Denoising vs. Original Data Encoding:
   Subject 1:
     - Original Corr (r) : 0.3171
     - Denoised Corr (r) : 0.3218
     - Improvement       : 1.49%
   Subject 2:
     - Original Corr (r) : 0.2176
     - Denoised Corr (r) : 0.1948
     - Improvement       : -10.50%
   Subject 3:
     - Original Corr (r) : 0.2162
     - Denoised Corr (r) : 0.2131
     - Improvement       : -1.41%
   Subject 4:
     - Original Corr (r) : 0.1952
     - Denoised Corr (r) : 0.1776
     - Improvement       : -8.98%
   Subject 5:
     - Original Corr (r) : 0.2332
     - Denoised Corr (r) : 0.2241
     - Improvement       : -3.92%
   Subject 6:
     - Original Corr (r) : 0.3163
     - Denoised Corr (r) : 0.3175
     - Improvement       : 0.39%
   Subject 7:
     - Original Corr (r) : 0.2264
     - Denoised Corr (r) : 0.1965
     - Improvement       : -13.21%
   Subject 8:
     - Original Corr (r) : 0.2228
     - Denoised Corr (r) : 0.1772
     - Improvement       : -20.45%

--- Script Finished ---
